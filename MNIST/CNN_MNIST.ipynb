{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    " \n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('data/',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\77699\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntf.reset_default_graph()函数:\\n用于清除默认图形堆栈并重置全局默认图形.默认图形是当前线程的一个属性。\\n该tf.reset_default_graph函数只适用于当前线程。\\n当一个tf.Session或者tf.InteractiveSession激活时调用这个函数会导致未定义的行为。\\n调用此函数后使用任何以前创建的tf.Operation或tf.Tensor对象将导致未定义的行为。\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "x = tf.placeholder(\"float\",shape = [None,28,28,1])   #28*28*1 不是784个像素点\n",
    "y_ = tf.placeholder(\"float\",shape = [None,10])\n",
    "'''\n",
    "tf.reset_default_graph()函数:\n",
    "用于清除默认图形堆栈并重置全局默认图形.默认图形是当前线程的一个属性。\n",
    "该tf.reset_default_graph函数只适用于当前线程。\n",
    "当一个tf.Session或者tf.InteractiveSession激活时调用这个函数会导致未定义的行为。\n",
    "调用此函数后使用任何以前创建的tf.Operation或tf.Tensor对象将导致未定义的行为。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义卷积核  5*5的卷积核 1维的原始输入图像 输出32通道 32种不同的核进行卷积得到32个不同的特征\n",
    "W_conv1 = tf.Variable(tf.truncated_normal([5,5,1,32], stddev=0.1))\n",
    "#偏置参数 用任意一个常数进行初始化\n",
    "b_conv1 = tf.Variable(tf.constant(.1,shape = [32]))\n",
    "#filter：卷积核 strides:步长 4个1 batchsize,h滑多少 ,w滑多少 ,通道数   SAME：可以自动补充\n",
    "h_conv1 = tf.nn.conv2d(input = x,filter = W_conv1,strides = [1,1,1,1],padding = 'SAME') + b_conv1\n",
    "#卷积一次 进行一次映射\n",
    "h_conv1 = tf.nn.relu(h_conv1)  \n",
    "#池化层 不会改变特征的个数\n",
    "#没有参数 ksize:batchsize,2*2区域,通道数 strides:步长 4个1 batchsize,h滑多少 ,w滑多少 ,通道数\n",
    "h_pool1 = tf.nn.max_pool(h_conv1,ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntruncated_normal()\\xa0 函数：\\n从截断的正态分布中取随机值。\\n生成的值服从具有指定平均值和标准偏差的正态分布，如果生成的值大于平均值两个标准差的值则丢弃重新选择。\\n\\ntf.nn.conv2d(imput,filter,strides,padding,use_cudnn_on_gpu=None,name=None)函数：\\n实现卷积的函数，搭建卷积神经网络的核心方法\\ninput:输入的图像\\nfilter:相当于CNN中的卷积核\\nstrides:卷积时在图像上的每一维的步长，[1,strides,strides,1]。\\npadding:设定卷积方式\\nuse_cudnn_on_gpu:bool类型，是否使用cudnn加速，默认为true\\n返回值：feature map,shape为[batch,h,w,channels]\\n\\ntf.nn.max_pool(value,ksize,strides,padding,name=None)函数：\\nCNN中最大池化操作\\nvalue:池化的输入，通常为feater map,\\nksize:池化窗口大小，一般为[1,h,w,1]\\nstrides:窗口在图像上的每一维的步长，[1,strides,strides,1]。\\npadding:设置池化方式\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "truncated_normal()  函数：\n",
    "从截断的正态分布中取随机值。\n",
    "生成的值服从具有指定平均值和标准偏差的正态分布，如果生成的值大于平均值两个标准差的值则丢弃重新选择。\n",
    "\n",
    "tf.nn.conv2d(imput,filter,strides,padding,use_cudnn_on_gpu=None,name=None)函数：\n",
    "实现卷积的函数，搭建卷积神经网络的核心方法\n",
    "input:输入的图像\n",
    "filter:相当于CNN中的卷积核\n",
    "strides:卷积时在图像上的每一维的步长，[1,strides,strides,1]。\n",
    "padding:设定卷积方式\n",
    "use_cudnn_on_gpu:bool类型，是否使用cudnn加速，默认为true\n",
    "返回值：feature map,shape为[batch,h,w,channels]\n",
    "\n",
    "tf.nn.max_pool(value,ksize,strides,padding,name=None)函数：\n",
    "CNN中最大池化操作\n",
    "value:池化的输入，通常为feater map,\n",
    "ksize:池化窗口大小，一般为[1,h,w,1]\n",
    "strides:窗口在图像上的每一维的步长，[1,strides,strides,1]。\n",
    "padding:设置池化方式\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 第二个卷积层 也可以写成函数\n",
    "def conv2d(x,W):\n",
    "\treturn tf.nn.conv2d(input = x, filter = W, strides = [1,1,1,1],padding = 'SAME')\n",
    "def max_pool_2x2(x):\t\n",
    "\treturn tf.nn.max_pool(x,ksize=[1,2,2,1],strides = [1,2,2,1],padding = 'SAME')\n",
    "#第二个卷积核\n",
    "W_conv2 = tf.Variable(tf.truncated_normal([5,5,32,64], stddev=0.1))\n",
    "b_conv2 = tf.Variable(tf.constant(.1,shape = [64]))\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#第一个全连接层\n",
    "W_fcl = tf.Variable(tf.truncated_normal([7*7*64,1024],stddev=0.1))\t#7*7*64转换为1024个特征\n",
    "b_fcl = tf.Variable(tf.constant(.1,shape = [1024]))\n",
    "#reshape将7*7*64拉伸成一长条,即h_pool2   -1自动取了一个合适的值，1\n",
    "h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])\n",
    "h_fcl = tf.nn.relu(tf.matmul(h_pool2_flat,W_fcl) + b_fcl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "tf.reshape(tensor,shape,name=None)函数：\n",
    "将tensor变化为shape的形式，shape为一个列表形式。\n",
    "防止过拟合\n",
    "'''\n",
    "#droput layer:杀死一部分神经元 防止过拟合\n",
    "#指定保留率\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fcl_drop = tf.nn.dropout(h_fcl,keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "tf.nn.dropout(x,keep_prob)函数\n",
    "防止过拟合，会随机扔掉一部分神经元。\n",
    "x:输入\n",
    "keep_prob:设置神经元被选中的概率，在初始化时keep_prob是一个占位符。\n",
    "'''\n",
    "#第二个全连接层\n",
    "W_fc2 = tf.Variable(tf.truncated_normal([1024,10],stddev=0.1))\t#\n",
    "b_fc2 = tf.Variable(tf.constant(.1,shape = [10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#最后一层\n",
    "y = tf.matmul(h_fcl_drop,W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-a01e07c65398>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#求损失值，指定优化器\n",
    "\n",
    "#用损失函数求交叉熵的损失\n",
    "crossEntropyLoss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_,logits = y))\n",
    "#指定优化器 AdamOptimizer可以自适应的调整学习率，比梯度下降好\n",
    "trainStep = tf.train.AdamOptimizer().minimize(crossEntropyLoss)\n",
    "#计算正确率\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracu 0.14\n",
      "step 100, training accuracu 0.92\n",
      "step 200, training accuracu 0.96\n",
      "step 300, training accuracu 0.96\n",
      "step 400, training accuracu 0.98\n",
      "step 500, training accuracu 0.94\n",
      "step 600, training accuracu 0.96\n",
      "step 700, training accuracu 0.98\n",
      "step 800, training accuracu 0.96\n",
      "step 900, training accuracu 0.98\n",
      "step 1000, training accuracu 0.98\n",
      "step 1100, training accuracu 1\n",
      "step 1200, training accuracu 0.98\n",
      "step 1300, training accuracu 0.96\n",
      "step 1400, training accuracu 0.98\n",
      "step 1500, training accuracu 1\n",
      "step 1600, training accuracu 1\n",
      "step 1700, training accuracu 0.98\n",
      "step 1800, training accuracu 0.9\n",
      "step 1900, training accuracu 1\n",
      "step 2000, training accuracu 0.98\n",
      "step 2100, training accuracu 1\n",
      "step 2200, training accuracu 1\n",
      "step 2300, training accuracu 0.98\n",
      "step 2400, training accuracu 0.98\n",
      "step 2500, training accuracu 1\n",
      "step 2600, training accuracu 0.98\n",
      "step 2700, training accuracu 1\n",
      "step 2800, training accuracu 1\n",
      "step 2900, training accuracu 1\n",
      "step 3000, training accuracu 0.98\n",
      "step 3100, training accuracu 0.98\n",
      "step 3200, training accuracu 1\n",
      "step 3300, training accuracu 1\n",
      "step 3400, training accuracu 0.96\n",
      "step 3500, training accuracu 0.98\n",
      "step 3600, training accuracu 1\n",
      "step 3700, training accuracu 1\n",
      "step 3800, training accuracu 0.96\n",
      "step 3900, training accuracu 1\n",
      "step 4000, training accuracu 1\n",
      "step 4100, training accuracu 1\n",
      "step 4200, training accuracu 0.98\n",
      "step 4300, training accuracu 0.98\n",
      "step 4400, training accuracu 0.98\n",
      "step 4500, training accuracu 1\n",
      "step 4600, training accuracu 1\n",
      "step 4700, training accuracu 1\n",
      "step 4800, training accuracu 1\n",
      "step 4900, training accuracu 0.98\n"
     ]
    }
   ],
   "source": [
    "#开始训练\n",
    "sess.run(tf.global_variables_initializer())\n",
    "batchsize = 50\n",
    "for i in range(5000):\n",
    "    batch = mnist.train.next_batch(batchsize)\n",
    "    trainingInputs = batch[0].reshape([batchsize,28,28,1])\t\t#输入大小必须为28*28*1\n",
    "    trainingLabels = batch[1]\n",
    "    if i%100 == 0:\n",
    "        trainAccuracy  = accuracy.eval(session = sess,feed_dict = {x:trainingInputs,y_:trainingLabels,keep_prob:0.5})\n",
    "        print(\"step %d, training accuracu %g\"%(i,trainAccuracy))\n",
    "    trainStep.run(session = sess,feed_dict={x:trainingInputs,y_:trainingLabels,keep_prob:0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
